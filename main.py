"""
Application RAG optimis√©e avec LangChain et Qdrant
Architecture modulaire avec gestion d'erreurs robuste et RAG complet.

Cette application impl√©mente un syst√®me RAG (Retrieval-Augmented Generation) complet :
1. Chargement et traitement de documents web
2. D√©coupage en chunks avec chevauchement
3. G√©n√©ration d'embeddings vectoriels
4. Stockage dans base vectorielle Qdrant
5. Recherche s√©mantique et g√©n√©ration de r√©ponses
"""

import getpass
import os
import bs4
from typing import List, Optional

# ============================================================================
# IMPORTS CONSOLID√âS
# ============================================================================

# Gestion des variables d'environnement
from dotenv import load_dotenv

# Mod√®les LangChain
from langchain.chat_models import init_chat_model
from langchain_openai import OpenAIEmbeddings

# Base vectorielle et stockage
from langchain_qdrant import QdrantVectorStore
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Templates et prompts
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document

# Client Qdrant et gestion des erreurs
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
from qdrant_client.http.exceptions import UnexpectedResponse


class RAGApplication:
    """
    Application RAG modulaire avec toutes les fonctionnalit√©s.
    
    Cette classe encapsule toute la logique RAG :
    - Configuration de l'environnement
    - Initialisation des mod√®les (LLM + Embeddings)
    - Gestion de la base vectorielle Qdrant
    - Chargement et traitement de documents
    - Recherche s√©mantique et g√©n√©ration de r√©ponses
    """
    
    def __init__(self):
        """Initialise l'application RAG avec des attributs par d√©faut."""
        self.llm = None                    # Mod√®le de langage (GPT-4o-mini)
        self.embeddings = None             # Mod√®le d'embeddings (text-embedding-3-small)
        self.vector_store = None           # Interface vers la base vectorielle Qdrant
        self.client = None                 # Client Qdrant pour la gestion des collections
        self.collection_name = "documents" # Nom de la collection dans Qdrant
        
    def setup_environment(self):
        """
        Configure l'environnement et les cl√©s API.
        
        Charge les variables d'environnement depuis le fichier .env
        et demande la cl√© API OpenAI si elle n'est pas trouv√©e.
        """
        load_dotenv()
        
        # V√©rification et demande de la cl√© API si n√©cessaire
        if not os.environ.get("OPENAI_API_KEY"):
            os.environ["OPENAI_API_KEY"] = getpass.getpass("Entrez votre cl√© API OpenAI: ")
    
    def initialize_models(self):
        """
        Initialise les mod√®les LLM et embeddings.
        
        Configure :
        - Mod√®le de chat GPT-4o-mini avec temp√©rature basse (0.1) pour des r√©ponses coh√©rentes
        - Mod√®le d'embeddings text-embedding-3-small (optimis√© co√ªt/performance)
        """
        print("üîß Initialisation des mod√®les...")
        
        # Mod√®le de chat optimis√© avec temp√©rature basse pour des r√©ponses coh√©rentes
        self.llm = init_chat_model(
            "gpt-4o-mini", 
            model_provider="openai",
            temperature=0.1  # R√©ponses plus d√©terministes
        )
        
        # Embeddings optimis√©s (moins co√ªteux que text-embedding-3-large)
        # Taille des vecteurs : 1536 dimensions
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        print("‚úÖ Mod√®les initialis√©s avec succ√®s")
    
    def setup_vector_store(self):
        """
        Configure la base de donn√©es vectorielle Qdrant.
        
        Cr√©e ou r√©cup√®re une collection Qdrant avec les param√®tres optimaux :
        - Distance COSINE pour la similarit√© s√©mantique
        - Taille des vecteurs adapt√©e au mod√®le d'embedding
        - Client en m√©moire pour le d√©veloppement (peut √™tre persist√© en production)
        """
        print("üóÑÔ∏è Configuration de la base vectorielle...")
        
        # Client Qdrant en m√©moire RAM (pour le d√©veloppement/test)
        # En production, utilisez une instance persistante : QdrantClient("localhost", port=6333)
        self.client = QdrantClient(":memory:")
        
        # Gestion robuste de la collection : cr√©ation si elle n'existe pas
        try:
            self.client.get_collection(self.collection_name)
            print(f"Collection '{self.collection_name}' trouv√©e")
        except ValueError:
            # Cr√©e la collection avec les param√®tres optimaux
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=1536,  # Taille pour text-embedding-3-small
                    distance=Distance.COSINE  # M√©trique de similarit√© s√©mantique
                )
            )
            print(f"Collection '{self.collection_name}' cr√©√©e")
        
        # Initialise l'interface vector store pour LangChain
        self.vector_store = QdrantVectorStore(
            client=self.client,
            collection_name=self.collection_name,
            embedding=self.embeddings,
        )
        
        print("‚úÖ Base vectorielle configur√©e")
    
    def load_and_process_documents(self, url: str) -> List[Document]:
        """
        Charge et traite les documents depuis une URL.
        
        Args:
            url (str): URL du document web √† charger
            
        Returns:
            List[Document]: Liste des chunks de documents trait√©s
            
        Processus :
        1. Chargement du document web avec BeautifulSoup
        2. Extraction du contenu pertinent (titre, en-t√™te, contenu)
        3. D√©coupage en chunks avec chevauchement
        4. Retour des chunks pr√™ts pour l'embedding
        """
        print(f"üìÑ Chargement du document depuis {url}...")
        
        # Configuration du loader web avec BeautifulSoup
        # Extraction s√©lective : seulement titre, en-t√™te et contenu principal
        bs4_strainer = bs4.SoupStrainer(class_=("post-title", "post-header", "post-content"))
        loader = WebBaseLoader(
            web_paths=(url,),
            bs_kwargs={"parse_only": bs4_strainer},  # Optimisation : parse seulement les √©l√©ments n√©cessaires
        )
        
        # Chargement du document
        docs = loader.load()
        print(f"Document charg√©: {len(docs[0].page_content)} caract√®res")
        
        # D√©coupage en chunks optimis√© pour RAG
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,      # Taille optimale pour les embeddings
            chunk_overlap=200,     # Chevauchement pour maintenir le contexte
            add_start_index=True,  # Suivi de la position dans le document original
        )
        
        splits = text_splitter.split_documents(docs)
        print(f"Document divis√© en {len(splits)} chunks")
        
        return splits
    
    def populate_vector_store(self, documents: List[Document]):
        """
        Ajoute les documents dans la base vectorielle.
        
        Args:
            documents (List[Document]): Liste des chunks √† ajouter
            
        Processus :
        1. G√©n√©ration automatique des embeddings pour chaque chunk
        2. Stockage dans la collection Qdrant
        3. V√©rification du nombre de documents ajout√©s
        """
        print("üîÑ Ajout des documents dans la base vectorielle...")
        
        # Ajout automatique avec g√©n√©ration d'embeddings
        self.vector_store.add_documents(documents)
        
        # V√©rification du stockage
        collection_info = self.client.get_collection(self.collection_name)
        print(f"‚úÖ {collection_info.points_count} documents ajout√©s")
    
    def search_documents(self, query: str, k: int = 3) -> List[Document]:
        """
        Recherche les documents pertinents pour une requ√™te.
        
        Args:
            query (str): Requ√™te de recherche
            k (int): Nombre de documents √† retourner (d√©faut: 3)
            
        Returns:
            List[Document]: Documents les plus pertinents
            
        Utilise la recherche par similarit√© cosinus dans l'espace vectoriel.
        """
        return self.vector_store.similarity_search(query, k=k)
    
    def generate_response(self, query: str, context_docs: List[Document]) -> str:
        """
        G√©n√®re une r√©ponse bas√©e sur les documents trouv√©s.
        
        Args:
            query (str): Question de l'utilisateur
            context_docs (List[Document]): Documents de contexte
            
        Returns:
            str: R√©ponse g√©n√©r√©e par le LLM
            
        Processus :
        1. Pr√©paration du contexte √† partir des documents
        2. G√©n√©ration du prompt avec template optimis√©
        3. Appel au LLM pour g√©n√©rer la r√©ponse
        """
        # Pr√©pare le contexte en concat√©nant les documents
        context = "\n\n".join([doc.page_content for doc in context_docs])
        
        # Template de prompt optimis√© pour RAG
        prompt = ChatPromptTemplate.from_template("""
Vous √™tes un assistant IA sp√©cialis√© dans l'analyse de documents.
R√©pondez √† la question en vous basant uniquement sur le contexte fourni.

Contexte:
{context}

Question: {question}

R√©ponse:""")
        
        # G√©n√©ration de la r√©ponse via le LLM
        messages = prompt.format_messages(context=context, question=query)
        response = self.llm.invoke(messages)
        
        return response.content
    
    def ask_question(self, query: str) -> tuple[str, List[Document]]:
        """
        Interface compl√®te RAG: recherche + g√©n√©ration.
        
        Args:
            query (str): Question de l'utilisateur
            
        Returns:
            tuple[str, List[Document]]: (r√©ponse g√©n√©r√©e, documents sources)
            
        Impl√©mente le pipeline RAG complet :
        1. Recherche s√©mantique des documents pertinents
        2. G√©n√©ration de r√©ponse bas√©e sur le contexte
        """
        print(f"\nü§î Question: {query}")
        
        # √âtape 1: Recherche des documents pertinents
        relevant_docs = self.search_documents(query)
        
        # √âtape 2: G√©n√©ration de la r√©ponse bas√©e sur le contexte
        response = self.generate_response(query, relevant_docs)
        
        return response, relevant_docs
    
    def run_demo(self):
        """
        D√©monstration compl√®te de l'application RAG.
        
        Ex√©cute le pipeline RAG complet :
        1. Configuration de l'environnement
        2. Initialisation des mod√®les
        3. Configuration de la base vectorielle
        4. Chargement et traitement des documents
        5. Tests avec diff√©rentes questions
        """
        try:
            # Configuration compl√®te de l'application
            self.setup_environment()
            self.initialize_models()
            self.setup_vector_store()
            
            # Chargement et traitement des documents
            url = "https://lilianweng.github.io/posts/2023-06-23-agent/"
            documents = self.load_and_process_documents(url)
            self.populate_vector_store(documents)
            
            # Tests RAG avec diff√©rentes questions
            questions = [
                "What is the main topic of this article?",
                "What are the key components of autonomous agents?",
                "How does planning work in AI agents?"
            ]
            
            print("\n" + "="*60)
            print("üéØ D√âMONSTRATION RAG COMPL√àTE")
            print("="*60)
            
            # Ex√©cution des tests
            for question in questions:
                response, sources = self.ask_question(question)
                
                print(f"\nüí¨ R√©ponse:\n{response}")
                print(f"\nüìö Sources utilis√©es: {len(sources)} documents")
                print("-" * 40)
            
            print("\nüéâ Application RAG compl√®te et fonctionnelle !")
            
        except Exception as e:
            print(f"‚ùå Erreur: {e}")
            import traceback
            traceback.print_exc()


def main():
    """
    Point d'entr√©e principal de l'application.
    
    Cr√©e une instance de RAGApplication et lance la d√©monstration.
    """
    print("üöÄ D√©marrage de l'application RAG")
    print("=" * 50)
    
    app = RAGApplication()
    app.run_demo()


if __name__ == "__main__":
    main()