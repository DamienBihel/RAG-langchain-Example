"""
Application RAG optimis√©e avec LangChain et Qdrant
Architecture modulaire avec gestion d'erreurs robuste et RAG complet.

Ce projet p√©dagogique impl√©mente un syst√®me RAG (Retrieval-Augmented Generation) complet :
1. Chargement et traitement de documents web
2. D√©coupage en chunks avec chevauchement
3. G√©n√©ration d'embeddings vectoriels
4. Stockage dans base vectorielle Qdrant
5. Recherche s√©mantique et g√©n√©ration de r√©ponses

CONCEPTS P√âDAGOGIQUES EXPLIQU√âS :
- RAG (Retrieval-Augmented Generation) : Technique qui combine recherche et g√©n√©ration
- Embeddings : Repr√©sentations vectorielles du texte pour la similarit√© s√©mantique
- Vector Store : Base de donn√©es sp√©cialis√©e pour stocker et rechercher des vecteurs
- Chunking : D√©coupage des documents en morceaux pour optimiser le traitement
- Similarity Search : Recherche par similarit√© cosinus dans l'espace vectoriel
- Prompt Engineering : Conception de prompts pour guider le LLM
"""

import getpass
import os
import bs4
from typing import List, Optional

# ============================================================================
# IMPORTS CONSOLID√âS - EXPLICATION DES BIBLIOTH√àQUES
# ============================================================================

# Gestion des variables d'environnement (s√©curit√© des cl√©s API)
from dotenv import load_dotenv

# Mod√®les LangChain - Framework pour applications LLM
from langchain.chat_models import init_chat_model
from langchain_openai import OpenAIEmbeddings

# Base vectorielle et stockage - Gestion des embeddings
from langchain_qdrant import QdrantVectorStore
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Templates et prompts - Conception de prompts pour le LLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document

# Client Qdrant et gestion des erreurs - Base de donn√©es vectorielle
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
from qdrant_client.http.exceptions import UnexpectedResponse


class RAGApplication:
    """
    Application RAG modulaire avec toutes les fonctionnalit√©s.
    
    Cette classe encapsule toute la logique RAG et sert d'exemple p√©dagogique
    pour comprendre l'architecture d'un syst√®me RAG complet.
    
    CONCEPTS P√âDAGOGIQUES :
    - Architecture modulaire : S√©paration des responsabilit√©s
    - Pipeline RAG : Flux de donn√©es du document √† la r√©ponse
    - Gestion d'erreurs robuste : Traitement des cas d'√©chec
    - Configuration flexible : Adaptation aux diff√©rents besoins
    
    Cette classe impl√©mente :
    - Configuration de l'environnement
    - Initialisation des mod√®les (LLM + Embeddings)
    - Gestion de la base vectorielle Qdrant
    - Chargement et traitement de documents
    - Recherche s√©mantique et g√©n√©ration de r√©ponses
    """
    
    def __init__(self):
        """
        Initialise l'application RAG avec des attributs par d√©faut.
        
        CONCEPTS P√âDAGOGIQUES :
        - Encapsulation : Regroupement des donn√©es et m√©thodes
        - √âtat de l'application : Variables d'instance pour maintenir l'√©tat
        - Configuration par d√©faut : Valeurs initiales raisonnables
        """
        # Mod√®le de langage (LLM) - Cerveau du syst√®me RAG
        self.llm = None                    # Mod√®le de langage (GPT-4o-mini)
        
        # Mod√®le d'embeddings - Conversion texte ‚Üí vecteurs
        self.embeddings = None             # Mod√®le d'embeddings (text-embedding-3-small)
        
        # Interface vers la base vectorielle - Stockage des embeddings
        self.vector_store = None           # Interface vers la base vectorielle Qdrant
        
        # Client Qdrant - Gestion directe de la base de donn√©es
        self.client = None                 # Client Qdrant pour la gestion des collections
        
        # Nom de la collection - Organisation des donn√©es
        self.collection_name = "documents" # Nom de la collection dans Qdrant
        
    def setup_environment(self):
        """
        Configure l'environnement et les cl√©s API.
        
        CONCEPTS P√âDAGOGIQUES :
        - S√©curit√© des cl√©s API : Protection des informations sensibles
        - Variables d'environnement : Configuration externalis√©e
        - Gestion d'erreurs : V√©rification de la pr√©sence des cl√©s
        
        Charge les variables d'environnement depuis le fichier .env
        et demande la cl√© API OpenAI si elle n'est pas trouv√©e.
        """
        # Chargement des variables d'environnement depuis .env
        load_dotenv()
        
        # V√©rification et demande de la cl√© API si n√©cessaire
        # BONNE PRATIQUE : Ne jamais hardcoder les cl√©s API dans le code
        if not os.environ.get("OPENAI_API_KEY"):
            os.environ["OPENAI_API_KEY"] = getpass.getpass("Entrez votre cl√© API OpenAI: ")
    
    def initialize_models(self):
        """
        Initialise les mod√®les LLM et embeddings.
        
        CONCEPTS P√âDAGOGIQUES :
        - Mod√®les de langage : Compr√©hension et g√©n√©ration de texte
        - Embeddings : Repr√©sentations vectorielles pour la similarit√©
        - Temp√©rature : Contr√¥le de la cr√©ativit√© vs coh√©rence
        - Optimisation co√ªt/performance : Choix de mod√®les √©quilibr√©s
        
        Configure :
        - Mod√®le de chat GPT-4o-mini avec temp√©rature basse (0.1) pour des r√©ponses coh√©rentes
        - Mod√®le d'embeddings text-embedding-3-small (optimis√© co√ªt/performance)
        """
        print("üîß Initialisation des mod√®les...")
        
        # Mod√®le de chat optimis√© avec temp√©rature basse pour des r√©ponses coh√©rentes
        # CONCEPT : La temp√©rature contr√¥le la "cr√©ativit√©" du mod√®le
        # - 0.0 = tr√®s d√©terministe, 1.0 = tr√®s cr√©atif
        self.llm = init_chat_model(
            "gpt-4o-mini", 
            model_provider="openai",
            temperature=0.1  # R√©ponses plus d√©terministes
        )
        
        # Embeddings optimis√©s (moins co√ªteux que text-embedding-3-large)
        # CONCEPT : Les embeddings convertissent le texte en vecteurs num√©riques
        # - Permettent la recherche s√©mantique (similarit√© de sens)
        # - Taille des vecteurs : 1536 dimensions pour text-embedding-3-small
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        print("‚úÖ Mod√®les initialis√©s avec succ√®s")
    
    def setup_vector_store(self):
        """
        Configure la base de donn√©es vectorielle Qdrant.
        
        CONCEPTS P√âDAGOGIQUES :
        - Base de donn√©es vectorielle : Stockage optimis√© pour les embeddings
        - Distance cosinus : M√©trique de similarit√© s√©mantique
        - Collection : Organisation des donn√©es par projet/domaine
        - Client en m√©moire vs persistant : Choix selon l'usage
        
        Cr√©e ou r√©cup√®re une collection Qdrant avec les param√®tres optimaux :
        - Distance COSINE pour la similarit√© s√©mantique
        - Taille des vecteurs adapt√©e au mod√®le d'embedding
        - Client en m√©moire pour le d√©veloppement (peut √™tre persist√© en production)
        """
        print("üóÑÔ∏è Configuration de la base vectorielle...")
        
        # Client Qdrant en m√©moire (pour le d√©veloppement/test)
        # CONCEPT : Base de donn√©es vectorielle sp√©cialis√©e
        # - Optimis√©e pour la recherche de similarit√©
        # - En production, utilisez une instance persistante : QdrantClient("localhost", port=6333)
        self.client = QdrantClient(":memory:")
        
        # Gestion robuste de la collection : cr√©ation si elle n'existe pas
        # CONCEPT : Gestion d'erreurs pour la robustesse
        try:
            self.client.get_collection(self.collection_name)
            print(f"Collection '{self.collection_name}' trouv√©e")
        except ValueError:
            # Cr√©e la collection avec les param√®tres optimaux
            # CONCEPT : Configuration de la base vectorielle
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=1536,  # Taille pour text-embedding-3-small
                    distance=Distance.COSINE  # M√©trique de similarit√© s√©mantique
                )
            )
            print(f"Collection '{self.collection_name}' cr√©√©e")
        
        # Initialise l'interface vector store pour LangChain
        # CONCEPT : Abstraction pour simplifier l'utilisation
        self.vector_store = QdrantVectorStore(
            client=self.client,
            collection_name=self.collection_name,
            embedding=self.embeddings,
        )
        
        print("‚úÖ Base vectorielle configur√©e")
    
    def load_and_process_documents(self, source: str, file_type: str = "auto") -> List[Document]:
        """
        Charge et traite les documents depuis une source.
        
        CONCEPTS P√âDAGOGIQUES :
        - Document Loaders : Chargement de diff√©rents formats
        - Text Splitting : D√©coupage intelligent des documents
        - Chunking : Optimisation pour les embeddings
        - M√©tadonn√©es : Informations contextuelles sur les chunks
        
        Args:
            source (str): Chemin vers le document (URL ou fichier)
            file_type (str): Type de fichier ("web", "pdf", "markdown", "auto")
            
        Returns:
            List[Document]: Liste des chunks de documents trait√©s
            
        Processus :
        1. Chargement du document selon son type
        2. D√©coupage en chunks avec chevauchement
        3. Enrichissement des m√©tadonn√©es
        4. Retour des chunks pr√™ts pour l'embedding
        """
        print(f"üìÑ Chargement du document depuis {source}...")
        
        # D√©tection automatique du type de fichier si non sp√©cifi√©
        if file_type == "auto":
            if source.startswith("http"):
                file_type = "web"
            elif source.endswith(".pdf"):
                file_type = "pdf"
            elif source.endswith(".md"):
                file_type = "markdown"
            else:
                file_type = "text"
        
        # Chargement selon le type de document
        # CONCEPT : Adapter le loader au type de document
        if file_type == "web":
            # Configuration du loader web avec BeautifulSoup
            # CONCEPT : Parsing HTML pour extraire le contenu pertinent
            bs4_strainer = bs4.SoupStrainer(class_=("post-title", "post-header", "post-content"))
            loader = WebBaseLoader(
                web_paths=(source,),
                bs_kwargs={"parse_only": bs4_strainer},  # Optimisation : parse seulement les √©l√©ments n√©cessaires
            )
        elif file_type == "pdf":
            # CONCEPT : Chargement de PDF avec extraction de texte
            from langchain_community.document_loaders import PyPDFLoader
            loader = PyPDFLoader(source)
        elif file_type == "markdown":
            # CONCEPT : Chargement de Markdown avec pr√©servation de la structure
            from langchain_community.document_loaders import UnstructuredMarkdownLoader
            loader = UnstructuredMarkdownLoader(source)
        else:
            # CONCEPT : Chargement de texte brut
            from langchain_community.document_loaders import TextLoader
            loader = TextLoader(source)
        
        # Chargement du document
        docs = loader.load()
        print(f"Document charg√©: {len(docs[0].page_content)} caract√®res")
        
        # D√©coupage en chunks optimis√© pour RAG
        # CONCEPT : Text Splitting pour optimiser les embeddings
        # - Chunks trop petits = perte de contexte
        # - Chunks trop grands = embeddings moins pr√©cis
        # - Chevauchement = maintien du contexte entre chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,      # Taille optimale pour les embeddings
            chunk_overlap=200,     # Chevauchement pour maintenir le contexte
            add_start_index=True,  # Suivi de la position dans le document original
        )
        
        splits = text_splitter.split_documents(docs)
        
        # Enrichir les m√©tadonn√©es des chunks
        # CONCEPT : M√©tadonn√©es pour la tra√ßabilit√© et le contexte
        for i, split in enumerate(splits):
            split.metadata["source"] = source
            split.metadata["file_type"] = file_type
            split.metadata["chunk_index"] = i
            split.metadata["total_chunks"] = len(splits)
        
        print(f"Document divis√© en {len(splits)} chunks")
        
        return splits
    
    def populate_vector_store(self, documents: List[Document]):
        """
        Ajoute les documents dans la base vectorielle.
        
        CONCEPTS P√âDAGOGIQUES :
        - G√©n√©ration d'embeddings : Conversion automatique texte ‚Üí vecteurs
        - Stockage vectoriel : Organisation optimis√©e pour la recherche
        - V√©rification : Contr√¥le de qualit√© du stockage
        
        Args:
            documents (List[Document]): Liste des chunks √† ajouter
            
        Processus :
        1. G√©n√©ration automatique des embeddings pour chaque chunk
        2. Stockage dans la collection Qdrant
        3. V√©rification du nombre de documents ajout√©s
        """
        print("üîÑ Ajout des documents dans la base vectorielle...")
        
        # Ajout automatique avec g√©n√©ration d'embeddings
        # CONCEPT : Pipeline automatique de traitement
        # - Chaque chunk est converti en embedding
        # - Les embeddings sont stock√©s avec leurs m√©tadonn√©es
        self.vector_store.add_documents(documents)
        
        # V√©rification du stockage
        # CONCEPT : Contr√¥le de qualit√© et monitoring
        collection_info = self.client.get_collection(self.collection_name)
        print(f"‚úÖ {collection_info.points_count} documents ajout√©s")
    
    def search_documents(self, query: str, k: int = 3) -> List[Document]:
        """
        Recherche les documents pertinents pour une requ√™te.
        
        CONCEPTS P√âDAGOGIQUES :
        - Recherche s√©mantique : Trouver des documents par similarit√© de sens
        - Similarit√© cosinus : M√©trique pour mesurer la similarit√© entre vecteurs
        - Param√®tre k : Nombre de documents √† retourner (trade-off pr√©cision/recouvrement)
        
        Args:
            query (str): Requ√™te de recherche
            k (int): Nombre de documents √† retourner (d√©faut: 3)
            
        Returns:
            List[Document]: Documents les plus pertinents
            
        Utilise la recherche par similarit√© cosinus dans l'espace vectoriel.
        """
        return self.vector_store.similarity_search(query, k=k)
    
    def generate_response(self, query: str, context_docs: List[Document]) -> str:
        """
        G√©n√®re une r√©ponse bas√©e sur les documents trouv√©s.
        
        CONCEPTS P√âDAGOGIQUES :
        - Prompt Engineering : Conception de prompts pour guider le LLM
        - Contexte : Information fournie au LLM pour la g√©n√©ration
        - Template : Structure r√©utilisable pour les prompts
        
        Args:
            query (str): Question de l'utilisateur
            context_docs (List[Document]): Documents de contexte
            
        Returns:
            str: R√©ponse g√©n√©r√©e par le LLM
            
        Processus :
        1. Pr√©paration du contexte √† partir des documents
        2. G√©n√©ration du prompt avec template optimis√©
        3. Appel au LLM pour g√©n√©rer la r√©ponse
        """
        # Pr√©pare le contexte en concat√©nant les documents
        # CONCEPT : Augmentation du prompt avec le contexte
        context = "\n\n".join([doc.page_content for doc in context_docs])
        
        # Template de prompt optimis√© pour RAG
        # CONCEPT : Prompt Engineering pour des r√©ponses de qualit√©
        # - Instructions claires pour le LLM
        # - S√©paration contexte/question
        # - Guidage pour la r√©ponse
        prompt = ChatPromptTemplate.from_template("""
Vous √™tes un assistant IA sp√©cialis√© dans l'analyse de documents.
R√©pondez √† la question en vous basant uniquement sur le contexte fourni.

Contexte:
{context}

Question: {question}

R√©ponse:""")
        
        # G√©n√©ration de la r√©ponse via le LLM
        # CONCEPT : Appel au mod√®le de langage avec le prompt format√©
        messages = prompt.format_messages(context=context, question=query)
        response = self.llm.invoke(messages)
        
        return response.content
    
    def ask_question(self, query: str) -> tuple[str, List[Document]]:
        """
        Interface compl√®te RAG: recherche + g√©n√©ration.
        
        CONCEPTS P√âDAGOGIQUES :
        - Pipeline RAG : Flux complet de traitement
        - Recherche + G√©n√©ration : Deux √©tapes distinctes
        - Transparence : Affichage des sources utilis√©es
        
        Args:
            query (str): Question de l'utilisateur
            
        Returns:
            tuple[str, List[Document]]: (r√©ponse g√©n√©r√©e, documents sources)
            
        Impl√©mente le pipeline RAG complet :
        1. Recherche s√©mantique des documents pertinents
        2. G√©n√©ration de r√©ponse bas√©e sur le contexte
        3. Affichage d√©taill√© des documents sources utilis√©s
        """
        print(f"\nü§î Question: {query}")
        
        # √âtape 1: Recherche des documents pertinents
        # CONCEPT : Retrieval - Trouver les informations pertinentes
        relevant_docs = self.search_documents(query)
        
        # √âtape 2: G√©n√©ration de la r√©ponse bas√©e sur le contexte
        # CONCEPT : Generation - Cr√©er une r√©ponse bas√©e sur le contexte
        response = self.generate_response(query, relevant_docs)
        
        # √âtape 3: Affichage d√©taill√© des documents sources
        # CONCEPT : Transparence et tra√ßabilit√©
        print(f"\nüìö DOCUMENTS UTILIS√âS DANS LE RAG ({len(relevant_docs)} documents):")
        print("=" * 80)
        
        for i, doc in enumerate(relevant_docs, 1):
            print(f"\nüìÑ Document #{i}")
            print("-" * 40)
            
            # Affichage des m√©tadonn√©es
            # CONCEPT : M√©tadonn√©es pour comprendre la source
            metadata = doc.metadata
            if metadata:
                print("üìå M√©tadonn√©es:")
                for key, value in metadata.items():
                    print(f"   ‚Ä¢ {key}: {value}")
            
            # Affichage du contenu (premiers 300 caract√®res)
            # CONCEPT : Aper√ßu du contenu utilis√©
            content = doc.page_content
            print(f"üìù Contenu ({len(content)} caract√®res):")
            print(f"   {content[:300]}...")
            if len(content) > 300:
                print(f"   ... (tronqu√©, {len(content) - 300} caract√®res suppl√©mentaires)")
        
        print("=" * 80)
        
        return response, relevant_docs
    
    def get_all_documents(self, limit: int = 100, with_vectors: bool = False) -> List[dict]:
        """
        R√©cup√®re tous les documents stock√©s dans la base vectorielle.
        
        Args:
            limit (int): Nombre maximum de documents √† r√©cup√©rer (d√©faut: 100)
            with_vectors (bool): Inclure les vecteurs dans la r√©ponse (d√©faut: False)
            
        Returns:
            List[dict]: Liste des documents avec leurs m√©tadonn√©es
        """
        print(f"üìã R√©cup√©ration des documents depuis la collection '{self.collection_name}'...")
        
        try:
            # R√©cup√©ration des points depuis Qdrant
            points = self.client.scroll(
                collection_name=self.collection_name,
                limit=limit,
                with_vectors=with_vectors,
                with_payload=True
            )
            
            documents = []
            for point in points[0]:  # points[0] contient la liste des points
                doc_info = {
                    "id": point.id,
                    "content": point.payload.get("page_content", ""),
                    "metadata": point.payload.get("metadata", {}),
                }
                if with_vectors:
                    doc_info["vector"] = point.vector
                documents.append(doc_info)
            
            print(f"‚úÖ {len(documents)} documents trouv√©s")
            return documents
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la r√©cup√©ration des documents: {e}")
            return []
    
    def display_documents(self, limit: int = 10, show_content: bool = True, max_content_length: int = 500):
        """
        Affiche les documents stock√©s dans la base vectorielle de fa√ßon format√©e.
        
        Args:
            limit (int): Nombre de documents √† afficher (d√©faut: 10)
            show_content (bool): Afficher le contenu des documents (d√©faut: True)
            max_content_length (int): Longueur maximale du contenu √† afficher (d√©faut: 500)
        """
        documents = self.get_all_documents(limit=limit)
        
        if not documents:
            print("Aucun document trouv√© dans la base vectorielle.")
            return
        
        print(f"\n{'='*80}")
        print(f"üìö DOCUMENTS DANS LA BASE VECTORIELLE ({len(documents)} documents)")
        print(f"{'='*80}")
        
        for i, doc in enumerate(documents, 1):
            print(f"\n{'‚îÄ'*60}")
            print(f"üìÑ Document #{i} (ID: {doc['id']})")
            print(f"{'‚îÄ'*60}")
            
            # Affichage des m√©tadonn√©es
            metadata = doc.get('metadata', {})
            if metadata:
                print("üìå M√©tadonn√©es:")
                for key, value in metadata.items():
                    print(f"   ‚Ä¢ {key}: {value}")
            
            # Affichage du contenu
            if show_content and doc.get('content'):
                content = doc['content']
                if len(content) > max_content_length:
                    content = content[:max_content_length] + "..."
                print(f"\nüìù Contenu:")
                print(f"   {content}")
            
            print(f"\n   Taille du contenu: {len(doc.get('content', ''))} caract√®res")
    
    def search_and_display(self, query: str, k: int = 5):
        """
        Recherche et affiche les documents pertinents avec leurs scores de similarit√©.
        
        Args:
            query (str): Requ√™te de recherche
            k (int): Nombre de documents √† retourner (d√©faut: 5)
        """
        print(f"\nüîç Recherche: '{query}'")
        print(f"{'='*60}")
        
        # Recherche avec scores
        results = self.vector_store.similarity_search_with_score(query, k=k)
        
        if not results:
            print("Aucun document pertinent trouv√©.")
            return
        
        print(f"üìä {len(results)} documents trouv√©s (tri√©s par pertinence)")
        print(f"{'='*60}")
        
        # Grouper par source
        sources_found = set()
        
        for i, (doc, score) in enumerate(results, 1):
            print(f"\nüèÜ R√©sultat #{i}")
            print(f"   Score de similarit√©: {score:.4f}")
            
            # M√©tadonn√©es avec mise en √©vidence de la source
            if doc.metadata:
                source = doc.metadata.get('source', 'Unknown')
                sources_found.add(source)
                print(f"   üìç Source: {source}")
                print("   üìå Autres m√©tadonn√©es:")
                for key, value in doc.metadata.items():
                    if key != 'source':
                        print(f"      ‚Ä¢ {key}: {value}")
            
            # Contenu (extrait)
            content = doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content
            print(f"   üìù Extrait:")
            print(f"      {content}")
            print(f"   üìè Taille totale: {len(doc.page_content)} caract√®res")
        
        # R√©sum√© des sources
        if sources_found:
            print(f"\nüìö Sources utilis√©es: {', '.join(sources_found)}")
    
    def get_sources_summary(self) -> dict:
        """
        Obtient un r√©sum√© des sources charg√©es dans la base vectorielle.
        
        Returns:
            dict: R√©sum√© avec sources, nombre de documents par source, etc.
        """
        documents = self.get_all_documents(limit=10000)
        
        sources_info = {}
        total_size = 0
        
        for doc in documents:
            source = doc['metadata'].get('source', 'Unknown')
            file_type = doc['metadata'].get('file_type', 'Unknown')
            
            if source not in sources_info:
                sources_info[source] = {
                    'file_type': file_type,
                    'chunks_count': 0,
                    'total_size': 0,
                    'chunk_indices': []
                }
            
            sources_info[source]['chunks_count'] += 1
            sources_info[source]['total_size'] += len(doc['content'])
            sources_info[source]['chunk_indices'].append(doc['metadata'].get('chunk_index', -1))
            total_size += len(doc['content'])
        
        return {
            'sources': sources_info,
            'total_sources': len(sources_info),
            'total_chunks': len(documents),
            'total_size': total_size
        }
    
    def display_sources(self):
        """Affiche un r√©sum√© format√© des sources charg√©es."""
        summary = self.get_sources_summary()
        
        print(f"\n{'='*80}")
        print(f"üìö R√âSUM√â DES SOURCES CHARG√âES")
        print(f"{'='*80}")
        
        print(f"\nüìä Statistiques globales:")
        print(f"   ‚Ä¢ Nombre de sources: {summary['total_sources']}")
        print(f"   ‚Ä¢ Nombre total de chunks: {summary['total_chunks']}")
        print(f"   ‚Ä¢ Taille totale: {summary['total_size']:,} caract√®res")
        
        if summary['sources']:
            print(f"\nüìë D√©tail par source:")
            print(f"{'‚îÄ'*80}")
            
            for source, info in summary['sources'].items():
                print(f"\nüìÑ {source}")
                print(f"   ‚Ä¢ Type: {info['file_type']}")
                print(f"   ‚Ä¢ Chunks: {info['chunks_count']}")
                print(f"   ‚Ä¢ Taille: {info['total_size']:,} caract√®res")
                print(f"   ‚Ä¢ Taille moyenne par chunk: {info['total_size'] // info['chunks_count']:,} caract√®res")
    
    def load_all_documents_from_folder(self, folder_path: str = "./documents") -> List[Document]:
        """
        Charge tous les documents du dossier sp√©cifi√©.
        
        Args:
            folder_path (str): Chemin vers le dossier contenant les documents
            
        Returns:
            List[Document]: Liste de tous les documents charg√©s
            
        Processus :
        1. Parcours r√©cursif du dossier
        2. D√©tection automatique du type de fichier
        3. Chargement et traitement de chaque document
        4. Retour de tous les documents combin√©s
        """
        print(f"üìÅ Chargement de tous les documents depuis {folder_path}...")
        
        all_documents = []
        
        try:
            import os
            import glob
            
            # Parcours r√©cursif du dossier
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    # Ignorer les fichiers syst√®me et temporaires
                    if file.startswith('.') or file.endswith(('.tmp', '.temp')):
                        continue
                    
                    file_path = os.path.join(root, file)
                    relative_path = os.path.relpath(file_path, folder_path)
                    
                    print(f"üìÑ Traitement de {relative_path}...")
                    
                    try:
                        # D√©tection du type de fichier
                        if file.endswith('.md'):
                            docs = self.load_and_process_documents(file_path, file_type="markdown")
                        elif file.endswith('.pdf'):
                            docs = self.load_and_process_documents(file_path, file_type="pdf")
                        elif file.endswith(('.html', '.htm')):
                            docs = self.load_and_process_documents(file_path, file_type="web")
                        else:
                            print(f"‚ö†Ô∏è Type de fichier non support√©: {file}")
                            continue
                        
                        # Ajouter les m√©tadonn√©es de source
                        for doc in docs:
                            doc.metadata["source_file"] = relative_path
                            doc.metadata["source_folder"] = folder_path
                        
                        all_documents.extend(docs)
                        print(f"‚úÖ {len(docs)} chunks ajout√©s depuis {relative_path}")
                        
                    except Exception as e:
                        print(f"‚ùå Erreur lors du traitement de {file}: {e}")
                        continue
            
            print(f"üìä Total: {len(all_documents)} chunks charg√©s depuis {folder_path}")
            return all_documents
            
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement du dossier {folder_path}: {e}")
            return []
    
    def run_demo(self):
        """
        D√©monstration compl√®te de l'application RAG.
        
        Ex√©cute le pipeline RAG complet :
        1. Configuration de l'environnement
        2. Initialisation des mod√®les
        3. Configuration de la base vectorielle
        4. Chargement et traitement des documents
        5. Tests avec diff√©rentes questions
        """
        try:
            # Configuration compl√®te de l'application
            self.setup_environment()
            self.initialize_models()
            self.setup_vector_store()
            
            # Chargement et traitement des documents depuis le dossier 'documents'
            documents = self.load_all_documents_from_folder("./documents")
            
            if not documents:
                print("‚ùå Aucun document trouv√© dans le dossier 'documents'")
                return
            
            self.populate_vector_store(documents)
            
            # Tests RAG avec des questions adapt√©es au contenu des documents
            questions = [
                "Qu'est-ce que le RAG (Retrieval Augmented Generation) ?",
                "Comment √©viter les biais dans un syst√®me d'IA ?",
                "Quel est le r√¥le d'un Centre d'Excellence en IA ?"
            ]
            
            print("\n" + "="*60)
            print("üéØ D√âMONSTRATION RAG COMPL√àTE")
            print("="*60)
            
            # Ex√©cution des tests
            for question in questions:
                response, sources = self.ask_question(question)
                
                print(f"\nüí¨ R√©ponse:\n{response}")
                print(f"\nüìö Sources utilis√©es: {len(sources)} documents")
                print("-" * 40)
            
            print("\nüéâ Application RAG compl√®te et fonctionnelle !")
            
        except Exception as e:
            print(f"‚ùå Erreur: {e}")
            import traceback
            traceback.print_exc()


# ============================================================================
# FONCTION PRINCIPALE - D√âMONSTRATION COMPL√àTE DU RAG
# ============================================================================

def main():
    """
    Fonction principale d√©montrant l'utilisation compl√®te du syst√®me RAG.
    
    CONCEPTS P√âDAGOGIQUES D√âMONTR√âS :
    - Initialisation compl√®te : Configuration de tous les composants
    - Pipeline RAG : Flux complet de traitement des documents
    - Gestion d'erreurs : Traitement robuste des cas d'√©chec
    - Interface utilisateur : Interaction claire et informative
    
    Cette fonction illustre :
    1. Configuration de l'environnement et des mod√®les
    2. Chargement et traitement de documents
    3. Stockage dans la base vectorielle
    4. Recherche s√©mantique et g√©n√©ration de r√©ponses
    5. Affichage d√©taill√© des r√©sultats
    """
    
    # ========================================================================
    # √âTAPE 1: INITIALISATION DU SYST√àME RAG
    # ========================================================================
    print("üöÄ Initialisation du syst√®me RAG...")
    
    # Cr√©ation de l'instance de l'application RAG
    # CONCEPT : Instanciation de la classe principale
    rag_app = RAGApplication()
    
    # Configuration de l'environnement (cl√©s API, etc.)
    # CONCEPT : S√©curit√© et configuration externalis√©e
    rag_app.setup_environment()
    
    # Initialisation des mod√®les (LLM + Embeddings)
    # CONCEPT : Mod√®les de langage et embeddings pour le traitement
    rag_app.initialize_models()
    
    # Configuration de la base vectorielle Qdrant
    # CONCEPT : Base de donn√©es sp√©cialis√©e pour les vecteurs
    rag_app.setup_vector_store()
    
    print("‚úÖ Syst√®me RAG initialis√© avec succ√®s!\n")
    
    # ========================================================================
    # √âTAPE 2: CHARGEMENT ET TRAITEMENT DES DOCUMENTS
    # ========================================================================
    print("üìö Chargement des documents...")
    
    # Sources de documents √† traiter
    # CONCEPT : Diversit√© des sources (web, fichiers locaux)
    documents_to_load = [
        # Document web (article technique)
        {
            "source": "https://lilianweng.github.io/posts/2023-06-23-agent/",
            "type": "web",
            "description": "Article sur les agents LLM"
        },
        # Document local Markdown (si disponible)
        {
            "source": "documents/rag_tutorial.md",
            "type": "markdown", 
            "description": "Tutoriel RAG local"
        },
        # Document local PDF (si disponible)
        {
            "source": "documents/ai_research.pdf",
            "type": "pdf",
            "description": "Recherche IA locale"
        }
    ]
    
    # Traitement de chaque document
    # CONCEPT : Pipeline de traitement pour chaque source
    for doc_info in documents_to_load:
        try:
            # Chargement et traitement du document
            # CONCEPT : Text splitting et pr√©paration pour embeddings
            documents = rag_app.load_and_process_documents(
                source=doc_info["source"],
                file_type=doc_info["type"]
            )
            
            # Ajout dans la base vectorielle
            # CONCEPT : G√©n√©ration d'embeddings et stockage
            rag_app.populate_vector_store(documents)
            
            print(f"‚úÖ Document '{doc_info['description']}' trait√© avec succ√®s")
            
        except Exception as e:
            # Gestion d'erreurs robuste
            # CONCEPT : Traitement des cas d'√©chec sans arr√™ter le processus
            print(f"‚ö†Ô∏è Erreur lors du traitement de '{doc_info['description']}': {e}")
            print("   Le document sera ignor√©, le syst√®me continue...")
            continue
    
    print(f"\nüìä Documents charg√©s dans la base vectorielle")
    
    # ========================================================================
    # √âTAPE 3: D√âMONSTRATION DES CAPACIT√âS RAG
    # ========================================================================
    print("\n" + "="*80)
    print("üéØ D√âMONSTRATION DES CAPACIT√âS RAG")
    print("="*80)
    
    # Questions de d√©monstration
    # CONCEPT : Diversit√© des requ√™tes pour tester le syst√®me
    demo_questions = [
        {
            "question": "Qu'est-ce qu'un agent LLM et comment fonctionne-t-il?",
            "description": "Question technique sur les agents LLM"
        },
        {
            "question": "Expliquez les concepts de RAG et d'embeddings vectoriels",
            "description": "Question p√©dagogique sur les concepts RAG"
        },
        {
            "question": "Quels sont les avantages et inconv√©nients des syst√®mes RAG?",
            "description": "Question d'analyse comparative"
        }
    ]
    
    # Traitement de chaque question
    # CONCEPT : Pipeline RAG complet pour chaque requ√™te
    for i, q_info in enumerate(demo_questions, 1):
        print(f"\nüîç Question #{i}: {q_info['description']}")
        print("-" * 60)
        
        try:
            # Appel du pipeline RAG complet
            # CONCEPT : Recherche + G√©n√©ration en une seule m√©thode
            response, sources = rag_app.ask_question(q_info["question"])
            
            # Affichage de la r√©ponse
            # CONCEPT : Interface utilisateur claire
            print(f"\nüí° R√âPONSE:")
            print(response)
            
        except Exception as e:
            # Gestion d'erreurs pour chaque question
            # CONCEPT : Robustesse du syst√®me
            print(f"‚ùå Erreur lors du traitement de la question: {e}")
            continue
    
    print("\n" + "="*80)
    print("‚úÖ D√©monstration RAG termin√©e!")
    print("="*80)


# ============================================================================
# POINT D'ENTR√âE ET GESTION D'ERREURS
# ============================================================================

if __name__ == "__main__":
    """
    Point d'entr√©e principal avec gestion d'erreurs compl√®te.
    
    CONCEPTS P√âDAGOGIQUES :
    - Point d'entr√©e : D√©marrage contr√¥l√© de l'application
    - Gestion d'erreurs globale : Protection contre les crashs
    - Messages informatifs : Feedback utilisateur clair
    - Structure main : Bonne pratique Python
    
    Cette section assure :
    - D√©marrage s√©curis√© de l'application
    - Gestion des erreurs non pr√©vues
    - Messages d'erreur informatifs
    - Sortie propre en cas de probl√®me
    """
    
    try:
        # Ex√©cution de la fonction principale
        # CONCEPT : Point d'entr√©e standard Python
        main()
        
    except KeyboardInterrupt:
        # Gestion de l'interruption utilisateur (Ctrl+C)
        # CONCEPT : Sortie propre lors de l'interruption
        print("\n\n‚ö†Ô∏è Interruption utilisateur d√©tect√©e")
        print("Arr√™t propre de l'application...")
        
    except Exception as e:
        # Gestion des erreurs non pr√©vues
        # CONCEPT : Robustesse et debugging
        print(f"\n‚ùå Erreur inattendue: {e}")
        print("V√©rifiez votre configuration et vos cl√©s API")
        print("Assurez-vous que tous les packages sont install√©s")
        
    finally:
        # Code de nettoyage (optionnel)
        # CONCEPT : Ressources et √©tat final
        print("\nüëã Application RAG ferm√©e")


# ============================================================================
# EXEMPLES D'UTILISATION ET BONNES PRATIQUES
# ============================================================================

"""
EXEMPLES D'UTILISATION P√âDAGOGIQUES
====================================

1. UTILISATION BASIQUE :
   ```python
   # Cr√©ation et configuration
   rag_app = RAGApplication()
   rag_app.setup_environment()
   rag_app.initialize_models()
   rag_app.setup_vector_store()
   
   # Chargement d'un document
   docs = rag_app.load_and_process_documents("mon_document.pdf", "pdf")
   rag_app.populate_vector_store(docs)
   
   # Question-r√©ponse
   response, sources = rag_app.ask_question("Ma question?")
   ```

2. UTILISATION AVANC√âE :
   ```python
   # Configuration personnalis√©e
   rag_app = RAGApplication()
   rag_app.collection_name = "mes_documents"  # Collection personnalis√©e
   
   # Chargement de multiples sources
   sources = [
       ("https://example.com/article", "web"),
       ("documents/rapport.pdf", "pdf"),
       ("data/notes.md", "markdown")
   ]
   
   for source, file_type in sources:
       docs = rag_app.load_and_process_documents(source, file_type)
       rag_app.populate_vector_store(docs)
   ```

3. RECHERCHE PERSONNALIS√âE :
   ```python
   # Recherche avec param√®tres personnalis√©s
   docs = rag_app.search_documents("ma requ√™te", k=5)  # Plus de r√©sultats
   
   # G√©n√©ration de r√©ponse personnalis√©e
   response = rag_app.generate_response("ma question", docs)
   ```

BONNES PRATIQUES P√âDAGOGIQUES
==============================

1. S√âCURIT√â :
   - ‚úÖ Utilisez des variables d'environnement pour les cl√©s API
   - ‚úÖ Ne committez jamais de cl√©s API dans le code
   - ‚úÖ Utilisez getpass() pour les entr√©es sensibles

2. GESTION D'ERREURS :
   - ‚úÖ G√©rez les exceptions pour chaque op√©ration critique
   - ‚úÖ Fournissez des messages d'erreur informatifs
   - ‚úÖ Continuez le traitement m√™me si un document √©choue

3. OPTIMISATION :
   - ‚úÖ Choisissez la taille de chunk appropri√©e (1000 caract√®res)
   - ‚úÖ Utilisez un chevauchement pour maintenir le contexte
   - ‚úÖ S√©lectionnez le bon mod√®le d'embedding selon vos besoins

4. MONITORING :
   - ‚úÖ Affichez les m√©tadonn√©es des documents utilis√©s
   - ‚úÖ Suivez le nombre de documents dans la base
   - ‚úÖ V√©rifiez la qualit√© des r√©ponses g√©n√©r√©es

5. ARCHITECTURE :
   - ‚úÖ Utilisez une architecture modulaire (classe RAGApplication)
   - ‚úÖ S√©parez les responsabilit√©s (chargement, recherche, g√©n√©ration)
   - ‚úÖ Rendez le code r√©utilisable et extensible

CONCEPTS CL√âS √Ä COMPRENDRE
==========================

1. RAG (Retrieval-Augmented Generation) :
   - Combine recherche s√©mantique et g√©n√©ration de texte
   - Am√©liore la pr√©cision des r√©ponses des LLM
   - Permet l'utilisation de connaissances externes

2. Embeddings Vectoriels :
   - Repr√©sentations num√©riques du texte
   - Permettent la recherche par similarit√© s√©mantique
   - Base de la recherche dans les syst√®mes RAG

3. Base de Donn√©es Vectorielle :
   - Stockage optimis√© pour les embeddings
   - Recherche rapide par similarit√© cosinus
   - Qdrant est une solution performante et open-source

4. Text Splitting :
   - D√©coupage intelligent des documents
   - Optimisation pour les embeddings
   - Maintien du contexte avec chevauchement

5. Prompt Engineering :
   - Conception de prompts pour guider les LLM
   - Int√©gration du contexte dans les prompts
   - Optimisation pour des r√©ponses de qualit√©

EXTENSIONS POSSIBLES
====================

1. Interface Web :
   - Ajout d'une interface Flask/FastAPI
   - Interface utilisateur avec Streamlit
   - API REST pour l'int√©gration

2. Fonctionnalit√©s Avanc√©es :
   - Support de multiples langues
   - Recherche hybride (s√©mantique + mots-cl√©s)
   - Filtrage par m√©tadonn√©es
   - Cache des embeddings

3. Monitoring et Analytics :
   - Logs d√©taill√©s des requ√™tes
   - M√©triques de performance
   - Dashboard de monitoring

4. Int√©grations :
   - Connexion √† des bases de donn√©es existantes
   - Int√©gration avec des syst√®mes de fichiers
   - Webhooks pour les mises √† jour automatiques

Ce projet sert de base solide pour comprendre et √©tendre les syst√®mes RAG.
Chaque composant est modulaire et peut √™tre adapt√© selon les besoins sp√©cifiques.
"""